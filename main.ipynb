{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":914551,"sourceType":"datasetVersion","datasetId":491586}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rag datasets trl bitsandbytes wandb Levenshtein","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8poKcefqSUaq","outputId":"a10dcd35-66f1-4526-d682-58a4168e9a53"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nfrom huggingface_hub import login\n\nhf_token = \"\"\nlogin(token=hf_token)\n","metadata":{"trusted":true,"id":"m31Ib6XgRhOW"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, pipeline, AutoModelForCausalLM\nimport json\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset, concatenate_datasets\n\nimport rag\nimport sqlparse\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport sqlite3\nfrom itertools import permutations\nfrom Levenshtein import distance as levenshtein_distance\nimport torch\nimport bitsandbytes\nfrom trl import setup_chat_format, DataCollatorForCompletionOnlyLM\nfrom peft import LoraConfig, AutoPeftModelForCausalLM,get_peft_model\nimport wandb","metadata":{"trusted":true,"id":"1okeaKANRhOW"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login --relogin 98aef964c1353cac148e079ff52355152f7935b3","metadata":{"trusted":true,"id":"F1CTjGquRhOW","outputId":"7594d76c-2649-4c59-8b4f-3cd78bea0109"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"Llama-3.2-1B-Instruct\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_to_train = \"meta-llama/Llama-3.2-1B-Instruct\"\nmodel_name = \"Llama-3.2-1B-Instruct\"\ndatasets_path = \"../spider_datasets\"\ndatabases_path = \"../spider_databases\"\ndataset_on_hub = \"rakshithjoseph/spider_data\"\nhf_token_read = \"****\"\nhf_token_write = \"****\"\nhf_username = \"rakshithjoseph\"\ntrain = True\nepochs = 1\nnum_train = -1\nevalu = True\ntest = True\nnb_rag_samples = 3\nquantize = True\nseed = 42\nref_correct = True\n","metadata":{"trusted":true,"id":"5ESksEOMSUar"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data preparation","metadata":{"id":"QQL7Tn2jWXqI"}},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_prep():\n    \n\n    #eos_token = \"<|im_end|>\"\n    system_message = \"\"\"You are a helpful assistant that generates SQL queries to answer questions about database tables.\nYou will receive: One or more SQL CREATE TABLE statements describing the structure of the tables.INSERT statements or a description of the data types for each column (you do not need to use the actual data).\nA natural language question about the data. Your task is to generate the correct and most efficient SQL query that answers the user's question, based only on the provided schema.\nOnly output the SQL query and nothing else. Do not explain the answer.\"\"\"\n    \n    user_message = \"\"\"Here is the schema of the tables you will use:\n    {schema}\n    \n    -- {question}\n    SELECT\"\"\"\n    dataset = load_dataset(dataset_on_hub)\n    def create_conv(sample):\n            q = sample[\"query\"]\n            lower_q = q.lower()\n            ind = lower_q.find(\"select\") + 6\n            q = q[ind:]\n            return {\n                    \"messages\": [\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": user_message.format(schema=sample[\"schema\"], question=sample[\"question\"])},\n                    {\"role\": \"assistant\", \"content\": q}\n                    ]\n                }\n    \n    print(\"Loading done...\")\n\n    train_dataset = dataset[\"train\"]\n    dev_dataset = dataset[\"dev\"]\n    test_dataset = dataset[\"test\"]\n\n    test_dataset_ids = test_dataset[\"db_id\"]\n    train_dataset_ids = train_dataset[\"db_id\"]\n\n\n    train_dataset = train_dataset.map(create_conv, remove_columns=train_dataset.features, batched=False)\n    dev_dataset = dev_dataset.map(create_conv, remove_columns=dev_dataset.features, batched=False)\n    test_dataset = test_dataset.map(create_conv, remove_columns=test_dataset.features, batched=False)\n\n    return train_dataset, dev_dataset, test_dataset, test_dataset_ids, train_dataset_ids\n\n\ndef data_loading():\n    print(\"Preparing the datasets\")\n    train_dataset, dev_dataset, test_dataset, test_dataset_ids, train_dataset_ids = data_prep()\n\n    train_dataset.to_json(f\"{datasets_path}/train_dataset.json\")\n    dev_dataset.to_json(f\"{datasets_path}/dev_dataset.json\")\n    test_dataset.to_json(f\"{datasets_path}/test_dataset.json\")\n    import pandas as pd\n    df = pd.DataFrame(test_dataset_ids)\n    df.to_json(f\"{datasets_path}/test_dataset_ids.json\")\n    df = pd.DataFrame(train_dataset_ids)\n    df.to_json(f\"{datasets_path}/train_dataset_ids.json\")\n\n    # Load the datasets\n    from datasets import load_dataset\n    train_dataset = load_dataset(\"json\", data_files=f\"{datasets_path}/train_dataset.json\")\n    dev_dataset = load_dataset(\"json\", data_files=f\"{datasets_path}/dev_dataset.json\")\n    test_dataset = load_dataset(\"json\", data_files=f\"{datasets_path}/test_dataset.json\")\n    test_dataset_ids = load_dataset(\"json\", data_files=f\"{datasets_path}/test_dataset_ids.json\")\n    train_dataset_ids = load_dataset(\"json\", data_files=f\"{datasets_path}/train_dataset_ids.json\")\n    print(\"Datasets are ready\")\n\n    return train_dataset['train'], dev_dataset['train'], test_dataset['train'], test_dataset_ids['train'], train_dataset_ids['train']","metadata":{"trusted":true,"id":"HAwMLEKCSUas"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluation","metadata":{"id":"7Z1h61LPWfF3"}},{"cell_type":"code","source":"def evaluation(model_name, test_dataset, test_dataset_ids):\n\n    nf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n    )\n    print(\"Loading model\")\n    model = AutoPeftModelForCausalLM.from_pretrained(\n    \"/kaggle/working/artifacts/Llama-3.2-1B-Instruct:v0\",\n    device_map=None,\n    torch_dtype=torch.float16,\n         ignore_mismatched_sizes=True,\n    quantization_config=nf4_config if quantize else None,\n    )\n    print(\"Model loaded. Model type: \", type(model))\n    tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/artifacts/Llama-3.2-1B-Instruct:v0\")\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n    def predict(sample):\n        prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][0:2], tokenize=False, add_generation_prompt=True)\n        schema = sample[\"messages\"][1][\"content\"]\n        outputs = pipe(prompt, max_new_tokens=256, do_sample=False, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n        predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n    \n        if predicted_answer.find(\";\") != -1:\n            predicted_answer = predicted_answer[:predicted_answer.find(\";\")+1]\n    \n        if ref_correct:\n            # print(f\"\\nFirst prediction: {predicted_answer}\")\n            cot_prompt = \" \" + predicted_answer\n            cot_prompt += \"\\nCorrect the SQL query above if necessary. You can look for issues in the table names, column names, or the query itself.\\nSELECT \"\n    \n            sample[\"messages\"][1][\"content\"] = sample[\"messages\"][1][\"content\"] + cot_prompt\n            prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][0:2], tokenize=False, add_generation_prompt=True)\n            schema = sample[\"messages\"][1][\"content\"]\n    \n            outputs = pipe(prompt, max_new_tokens=256, do_sample=False, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n            predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n    \n            # print(f\"Chain of thought prediction: {predicted_answer}\\n\")\n        low_pred = predicted_answer.lower()\n        if not low_pred.startswith(\"select\") and not low_pred.startswith(\" select\"):\n            predicted_answer = \"SELECT \" + predicted_answer\n    \n        if not check_table_names(predicted_answer, schema):\n            predicted_answer = correct_query(predicted_answer, schema)\n            #print(f\"Corrected table names in the predicted answer: {predicted_answer}\")\n    \n    \n        return predicted_answer\n    em_match = []\n    comp_match=[]\n    none_rate = 0\n    database_fails = 0\n    component_keys = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'JOIN']\n    total_correct = {k: 0 for k in component_keys}\n    total_present = {k: 0 for k in component_keys}\n    all_results=[]\n    for i in range(len(test_dataset)):\n        db_id = test_dataset_ids['0'][0][f\"{i}\"]\n        db_path = f\"{databases_path}/test_database/{db_id}/{db_id}.sqlite\"\n        prediction = predict(test_dataset[i])\n        p_list=extract_sql_queries(prediction)\n        prediction=p_list[len(p_list)-1]\n        true_pred = \"SELECT \" + test_dataset[i][\"messages\"][2][\"content\"]\n        true_pred.replace(\"<|im_end|>\", \"\")\n        em_match.append(exact_match(true_pred, prediction))\n        res=component_matching([true_pred], [prediction])\n        all_results.append(res)\n        comp_match.append(res['average_accuracy'])\n\n        exact_match_accuracy = sum(em_match)/len(em_match)\n        comp_match_accuracy=sum(comp_match)/len(comp_match)\n        completion_rate = (i+1)/len(test_dataset)\n        msg = \"Completion rate: {0}, Component Matching: {1}, Exact Match Accuracy: {2}\".format(completion_rate,\n                                                                                                    comp_match_accuracy,\n                                                                                                    exact_match_accuracy)\n        sys.stdout.write(\"\\r\" + msg)\n        sys.stdout.flush()\n    \n    for res in all_results:\n        for k in component_keys:\n            val = res['component_accuracy'][k]\n            if val is not None:\n                total_present[k] += 1\n                if val == 1.0:\n                    total_correct[k] += 1\n    \n    # Final computation\n    final_accuracy = {\n        k: (total_correct[k] / total_present[k]) if total_present[k] > 0 else None\n        for k in component_keys\n    }\n    print(final_accuracy)\n    return exact_match_accuracy, comp_match_accuracy\n\n\n\n\n","metadata":{"trusted":true,"id":"oNO9TvfDSUat","outputId":"04af1c04-b45e-4b36-f09a-42beaea59c8b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training","metadata":{"id":"wjcYQqgvXLtj"}},{"cell_type":"code","source":"def extract_sql_queries(text):\n    queries = re.findall(r\"```sql(.*?)```\", text, re.DOTALL)\n    if not queries:\n        queries = re.findall(r\"(SELECT .*?)(?:\\n|$)\", text, re.IGNORECASE)\n    return [q.strip() for q in queries]\n\n\ndef train(model_name, train_dataset):\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\n    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_to_train,\n        device_map=\"auto\",\n        # attn_implementation=\"flash_attention_2\",\n        torch_dtype=torch.bfloat16,\n        quantization_config=bnb_config,\n        trust_remote_code=True,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_to_train,truncation=True, max_length=2048)\n    tokenizer.padding_side = 'right'\n    tokenizer.chat_template=None\n    tokenizer.truncation=True\n\n    model, tokenizer = setup_chat_format(model, tokenizer)\n    peft_config = LoraConfig(\n            lora_alpha=128,\n            lora_dropout=0.05,\n            r=8,\n            bias=\"none\",\n            target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n            task_type=\"CAUSAL_LM\", # for encoder only models like Llama or GPT\n    )\n\n\n\n\n\n    train_args = SFTConfig(\n        output_dir=model_name,\n        num_train_epochs=epochs,           # number of training epochs\n        per_device_train_batch_size=8,          # batch size per device during training\n        gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n        gradient_checkpointing=True,            \n        optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n        logging_steps=10,\n        log_level=\"info\",\n        save_strategy=\"steps\",                  \n        learning_rate=3e-4,                     # learning rate\n        bf16=True,\n        logging_dir=\"./logs\",\n        report_to=\"wandb\",\n        run_name=\"NLP_project\",\n        tf32=False,\n        max_grad_norm=0.3,\n        warmup_ratio=0.03,\n        max_seq_length=1024,\n        lr_scheduler_type=\"cosine\",             # learning rate scheduler type\n        push_to_hub=False,                       # push model to hub at every save\n        eval_strategy=\"steps\" if eval else 'no',\n        eval_steps=50 if eval else None,\n        packing=True,\n        dataset_kwargs={\n            \"add_special_tokens\": False,\n            \"append_concat_token\": False,\n            \"max_length\": 1024,\n            \"truncation\": True,\n        }\n    )\n\n    train_dataset = train_dataset.shuffle(seed=42)\n    eos_token = tokenizer.eos_token\n    print(\"EOS token id: \", eos_token)\n    trainer = SFTTrainer(\n        model=model,\n        args=train_args,\n        train_dataset=train_dataset,\n        eval_dataset=dev_dataset if evalu else None,\n        peft_config=peft_config\n\n    )\n\n    print(\"Training has started!!\")\n\n    trainer.train()\n\n    trainer.save_model()\n    artifact = wandb.Artifact(\"Llama-3.2-1B-Instruct\", type=\"model\")\n    artifact.add_dir(\"/kaggle/working/Llama-3.2-1B-Instruct\")\n    wandb.log_artifact(artifact)\n    print(\"Training has ended!!\")\n\n    del model\n    del trainer\n    torch.cuda.empty_cache()\n\n    return\n\n\n","metadata":{"trusted":true,"id":"p2BkCpyOSUa0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Component matching","metadata":{"id":"jtf3rMAMXgBr"}},{"cell_type":"code","source":"def normalize_columns(component_str):\n    if not component_str:\n        return None\n    cols = [c.strip().lower() for c in component_str.split(',')]\n    return sorted(cols)\ndef clean_sqlw(s):\n    return re.sub(r\"\\s\", \"\", s.strip().lower())\ndef clean_sql(s):\n    return re.sub(r\"\\s+\", \" \", s.strip().lower())\ndef extract_components(query):\n    components = {\n        'SELECT': None,\n        'FROM': None,\n        'WHERE': None,\n        'GROUP BY': None,\n        'ORDER BY': None,\n        'JOIN': None\n    }\n\n    parsed = sqlparse.parse(query)\n    if not parsed:\n        return components\n\n    stmt = parsed[0]\n    tokens = [t for t in stmt.tokens if not t.is_whitespace]\n\n    idx = 0\n    while idx < len(tokens):\n        token = tokens[idx]\n        val = token.value.upper()\n\n\n        if token.ttype is sqlparse.tokens.DML and val == 'SELECT':\n            components['SELECT'] = normalize_columns(str(tokens[idx + 1])) if idx + 1 < len(tokens) else None\n        elif token.ttype is sqlparse.tokens.Keyword and val == 'FROM':\n            components['FROM'] = clean_sql(str(tokens[idx + 1])) if idx + 1 < len(tokens) else None\n        elif val.startswith('WHERE'):\n            parts = re.split(r'(?i)\\b(WHERE)\\b', val)\n            components['WHERE'] = clean_sqlw(parts[2]) if idx < len(tokens) else None\n\n        elif val.startswith('GROUP BY'):\n            components['GROUP BY'] = clean_sql(str(tokens[idx + 1])) if idx + 1 < len(tokens) else None\n        elif val.startswith('ORDER BY'):\n            components['ORDER BY'] = clean_sql(str(tokens[idx + 1])) if idx + 1 < len(tokens) else None\n        elif 'JOIN' in val:\n            components['JOIN'] = clean_sql(str(tokens[idx + 1])) if idx + 1 < len(tokens) else None\n\n        idx += 1\n\n    return components\n\ndef component_matching(y_true, y_pred):\n    keys = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'ORDER BY', 'JOIN']\n    correct_counts = {k: 0 for k in keys}\n    present_counts = {k: 0 for k in keys}\n\n    for true_q, pred_q in zip(y_true, y_pred):\n        true_comp = extract_components(true_q)\n        pred_comp = extract_components(pred_q)\n        for k in keys:\n            if true_comp[k] is not None:\n                present_counts[k] += 1\n                if true_comp[k] == pred_comp[k]:\n                    correct_counts[k] += 1\n\n    acc = {\n        k: (correct_counts[k] / present_counts[k]) if present_counts[k] > 0 else None\n        for k in keys\n    }\n    valid_acc = [v for v in acc.values() if v is not None]\n    avg = sum(valid_acc) / len(valid_acc) if valid_acc else 0.0\n    return {\n        'component_accuracy': acc,\n        'average_accuracy': avg\n    }\n","metadata":{"id":"nYQgpNAQVSbI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_table_names(schema):\n    table_names = []\n    schema = schema.lower()\n    schema = schema.split('\\n')\n    for line in schema:\n        if 'create table if not exists' in line:\n            table_name = line.split()[5].translate(str.maketrans('', '', '\"\\',`;()'))\n            table_names.append(table_name)\n        elif 'create table' in line:\n            table_name = line.split()[2].translate(str.maketrans('', '', '\"\\',`;()'))\n            table_names.append(table_name)\n    return table_names\n\n\ndef similarity_search(wrong_table, tables):\n    min_distance = -1\n    similar_table = ''\n    for table in tables:\n        distance = levenshtein_distance(wrong_table, table)\n        if distance < min_distance or min_distance == -1:\n            min_distance = distance\n            similar_table = table\n    return similar_table\n\ndef get_table_names_from_query(query):\n    table_names = []\n    query = query.lower().split()\n\n    for i, word in enumerate(query):\n        if word in ['from', 'join']:\n            if i + 1 < len(query):\n                table_name = query[i + 1]\n                if len(table_name) > 2 and not (table_name[0] == '(' or table_name[1:2] == '(' or table_name[2:3] == '('):\n\n                    for ch in [\",\", \"'\", '\"', '`', ';', '(', ')']:\n                        table_name = table_name.replace(ch, '')\n                    table_names.append(table_name)\n                else:\n                    table_names.append(table_name)\n    return table_names\n\n\ndef check_table_names(query, schema):\n    table_names = get_table_names(schema)\n    query_table_names = get_table_names_from_query(query)\n    for query_table_name in query_table_names:\n        if query_table_name not in table_names:\n            return False\n    return True\n\ndef correct_query(query, schema):\n\n    table_names = get_table_names(schema)\n    query_table_names = get_table_names_from_query(query)\n    #print(\"query table names: \", query_table_names)\n    #print(\"true table names: \", table_names)\n    for query_table_name in query_table_names:\n        if query_table_name not in table_names:\n            similar_table = similarity_search(query_table_name, table_names)\n            similar_table = similar_table.replace('\"', '')\n            low_query = query.lower()\n            split_low_query = low_query.split()\n            split_query = query.split()\n            try:\n                split_query[split_low_query.index(query_table_name)] = similar_table\n            except:\n                print(\"Less error\")\n            query = ' '.join(split_query)\n    return query\n\ndef exact_match(y_true, y_pred):\n    y_true = y_true.lower()\n    y_pred = y_pred.lower()\n\n    y_true = re.sub(r'\\s+', '', y_true)\n    y_pred = re.sub(r'\\s+', '', y_pred)\n    y_true = y_true.replace(';', '')\n    y_pred = y_pred.replace(';', '')\n    y_true = y_true.replace(',', '')\n    y_pred = y_pred.replace(',', '')\n    y_true = re.sub(r't[0-6]\\.', '', y_true)\n    y_pred = re.sub(r't[0-6]\\.', '', y_pred)\n    if y_true == y_pred:\n        return 1\n    else:\n        return 0\n\n","metadata":{"trusted":true,"id":"B85pE3zGSUa4","outputId":"9f6cd29f-482c-415f-8113-e3507c29d245"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_dataset, dev_dataset, test_dataset, test_dataset_ids, train_dataset_ids = data_loading()\n\n    if train:\n        # train the model\n        print(\"Training the model\")\n        train(model_name, train_dataset)\n\n    if test:\n        print(\"Evaluating the model\")\n        exact_accuracy,comp = evaluation(model_name, test_dataset, test_dataset_ids)\n\n        print(f\"Component Matching Accuracy: {comp}\")\n        print(f\"Exact Match Accuracy: {exact_accuracy}\")\n        print(\"--------done--------\")\n\n\n\n","metadata":{"trusted":true,"colab":{"referenced_widgets":["c2ef00c75935490cac8ca31f0699ef23","10237e3eb09f4a5dada8cf5e26fff2d7","664181dcf0ec41d69efb1a9a2bee696c","8ddcae862af045398b378452a4352479","e2457f5cee104a078b974e943b59f426","2b7d311be66341368bac75357399f5d8","b75da3abd5244863be89942bb50c95a5","b1388598a18542859b51dd46f3cd56d5","26ab6d6ebfd74c09b125febafb6be42a","82fcb6fde2ac4e90a536b7968c5c3d29","b7c50c6fa1d24aba995ac1bfdac0c37d","13fe3eaefd8640f3b43faa3d621dfb4b","ddfc8d7355d742f892b95b09ec78120c","afbf8f83c7204e8295132373e5d7d124","24298692e05a47adaa0ad39936397fad","19af3cd6d8de4a2c9432f1c8e8c08671","e3c8b47355c8432e8730d074c89ea038","199cbcf7c7854d339615375954c6f7d0","e50ae6e5bf1c4f23a02e8a61a08401b6","75fcf4eced8e4c0185d563079d5631f8","41d3ec8d2b1649d7a900b9613b1ec7be","2896029ca97943fda111610ebb39b88d","bf794b155a304c838b44d756023bb5c4","ced91cb5a8a64a9fbe2a6e9f7f50e467","de6dcd9b0097411b80130309205a6e3c","50ce8d68a7e644df929b155a48d8b21c","40560afba6b84ba3b5c85cb63c9ab974","0b6d1442387f41c2999019f0fa521ca2","8d3d7c66c5b74e2c97d55b0935dc9f5c","4ca3fd21b8b249fc948379a956dd487e","ae23592371fe4e7baec1b30753fbc61d","083f73e41e5e43c18aa3970f4a2ce566"]},"id":"vrM0yDHXRhOY","outputId":"c45ff1f8-b6df-4619-b457-cb5b26a23219","_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Inferencing: Loading Model from Wandb**","metadata":{"id":"iLruQy9hRhOY"}},{"cell_type":"code","source":"import wandb\nrun = wandb.init()\nartifact = run.use_artifact('pawankr-cse-iit-delhi/huggingface/Llama-3.2-1B-Instruct:v0', type='model')\n\nmodel_dir = artifact.download()\nprint(f\"Model downloaded to: {model_dir}\")","metadata":{"trusted":true,"id":"tkmnQ2kFRhOZ","outputId":"abe3bb71-b578-465b-e6a2-f7c3d68551e6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nf4_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_use_double_quant=True,\nbnb_4bit_compute_dtype=torch.bfloat16\n)\nprint(\"Loading model\")\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n\"/kaggle/working/artifacts/Llama-3.2-1B-Instruct:v0\",\ndevice_map=None,\ntorch_dtype=torch.float16,\n     ignore_mismatched_sizes=True,\nquantization_config=nf4_config if quantize else None,\n)\nprint(\"Model loaded. Model type: \", type(model))\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/artifacts/Llama-3.2-1B-Instruct:v0\")\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from random import randint\nrand_idx = randint(0, len(test_dataset)+1)\n\nprint(f\"EOS token: {pipe.tokenizer.eos_token}\")\n\n# Test on sample\nprompt = pipe.tokenizer.apply_chat_template(test_dataset[rand_idx][\"messages\"][0:2], tokenize=False, add_generation_prompt=False)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\nprint(f\"Natural language request:\\n{test_dataset[rand_idx]['messages'][1]['content']}\\n\\n\")\nprint(f\"Original Answer:\\n{test_dataset[rand_idx]['messages'][2]['content']}\\n\\n\")\ngenerated_answer = outputs[0]['generated_text'][len(prompt):].strip()\nprint(f\"First generated Answer:\\n{generated_answer}\\n\\n\")\nif generated_answer.find(\";\") != -1:\n    generated_answer = generated_answer[:generated_answer.find(\";\")+1]\nprint(f\"Generated Answer:\\n{generated_answer}\\n\\n\")","metadata":{"trusted":true,"id":"fu_bOdWDRhOZ","outputId":"3165a7fb-2878-4b9d-c440-f44bc67e8bdf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}